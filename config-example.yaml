# Discord settings:

bot_token: YOUR_DISCORD_BOT_TOKEN # Example: MTA2...
client_id: YOUR_BOTS_CLIENT_ID    # Example: 106... (For invite link generation)
status_message:                   # Example: Chatting with LLMs

# Message processing settings:
max_text: 100000  # Max characters per message to consider
max_images: 5     # Max images per message (if model supports vision)
max_messages: 25  # Max messages in conversation history

use_plain_responses: false # Set to true for plain text replies instead of embeds
allow_dms: true          # Allow the bot to be used in Direct Messages

# Permissions: Set IDs to allow/block specific users, roles, or channels. Leave lists empty to allow all.
permissions:
  users:
    allowed_ids: [] # e.g., [123456789...]
    blocked_ids: []
  roles:
    allowed_ids: [] # e.g., [987654321...]
    blocked_ids: []
  channels: # Checks channel, parent forum channel, and category
    allowed_ids: [] # e.g., [112233445...]
    blocked_ids: []


# Rate Limiting: Control how often users can interact with the bot.
rate_limits:
  enabled: true       # Set to false to disable rate limiting entirely
  user_limit: 5       # Max number of requests per user within the period
  user_period: 60     # Time period in seconds
  global_limit: 100    # Max total requests for the bot within the global period
  global_period: 60    # Time period in seconds for the global limit
  # --- Reasoning Model Rate Limits (if multimodel enabled) ---
  reasoning_user_limit: 2    # Max reasoning requests per user within the period
  reasoning_user_period: 300   # Time period in seconds for reasoning limit
  reasoning_global_limit: 2 # Optional: Max total reasoning requests globally
  reasoning_global_period: 61 # Optional: Time period for global reasoning limit

  # admin_bypass: true # Optional: Allow users with admin permissions to bypass limits

# --- Persistent Memory Settings ---
memory:
  enabled: true                       # Set to true to enable persistent memory
  database_path: "llmcord_memory.db"  # Path to the SQLite database file
  # How to inject memory into the prompt:
  # "system_prompt_prefix": Adds memory before the main system prompt. (Recommended)
  # "user_message_prefix": Adds memory as a separate user message after the system prompt.
  prompt_injection_method: "system_prompt_prefix"
  memory_prefix: "[User Memory/Notes]:\n" # Text prepended to the memory content in the prompt
  max_memory_length: 1500             # Max characters allowed for stored memory per user
  # LLM-suggested memory section removed (now integrated into main response)

  # --- Memory Condensation Settings ---
  condensation_threshold_percent: 80 # Condense memory if it exceeds this percentage of max_memory_length
  condensation_target_buffer: 100  # How many characters below max_length the LLM should aim for
  condensation_prompt: >
    Please summarize and condense the following notes, removing redundancy
    and keeping the most important points. Aim for a maximum length of
    around {target_len} characters, but do not exceed {max_len} characters.\n\n
    NOTES:\n```\n{current_memory}\n```\n\nCONDENSED NOTES:
  # Memory Merge Settings removed (no longer needed)
# --- Multimodel Settings (Optional) ---
# Allows the bot to switch to a more powerful model for complex tasks.
multimodel:
  enabled: false              # Set to true to enable switching to the reasoning model
  reasoning_model: "openai/gpt-4o" # The model to use for complex reasoning tasks (e.g., openai/gpt-4o, google-gemini/gemini-1.5-pro-latest)
  reasoning_signal: "[USE_REASONING_MODEL]" # The exact text the default model should output to trigger the switch
  notify_user: true           # Set to true to send a "Thinking deeper..." message when switching
  # Optional: Override API parameters specifically for the reasoning model
  reasoning_extra_api_parameters:
    # max_tokens: 8192
    # temperature: 0.5
  reasoning_signal: "[USE_REASONING_MODEL]" # The exact text the default model should output to trigger the switch
  notify_user: true           # Set to true to send a "Thinking deeper..." message when switching

# LLM settings:

providers:
  openai:
    base_url: https://api.openai.com/v1
    api_key: YOUR_OPENAI_API_KEY # sk-...
  x-ai:
    base_url: https://api.x.ai/v1
    api_key: YOUR_XAI_API_KEY
  mistral:
    base_url: https://api.mistral.ai/v1
    api_key: YOUR_MISTRAL_API_KEY
  groq:
    base_url: https://api.groq.com/openai/v1
    api_key: YOUR_GROQ_API_KEY # gsk_...
  openrouter: # Can proxy many models including Gemini, useful for OpenAI format
    base_url: https://openrouter.ai/api/v1
    api_key: YOUR_OPENROUTER_API_KEY # sk-or-...
  google-gemini: # Uses the native Google Gemini API (requires google-generativeai library)
    api_key: YOUR_GOOGLE_API_KEY # From Google AI Studio / GCP
    # base_url: Not needed, handled by google-generativeai library
  ollama:
    base_url: http://localhost:11434/v1
    # api_key: Not typically required for local Ollama
  lmstudio:
    base_url: http://localhost:1234/v1
  vllm:
    base_url: http://localhost:8000/v1
  oobabooga:
    base_url: http://localhost:5000/v1
  jan:
    base_url: http://localhost:1337/v1

# Select the provider and model. Format: provider_name/model_name
# Examples: openai/gpt-4o, google-gemini/gemini-1.5-pro-latest, ollama/llama3
model: google-gemini/gemini-1.5-pro-latest

# Optional parameters passed directly to the LLM API (must be supported by the provider/model)
# Check provider documentation for available parameters.
extra_api_parameters:
  # Common parameters (OpenAI compatible):
  max_tokens: 4096 # Max tokens for the *response*
  temperature: 0.8
  # top_p: 1.0
  # frequency_penalty: 0.0
  # presence_penalty: 0.0
  # Google Gemini specific generation_config parameters:
  # candidate_count: 1 # Gemini equivalent for 'n' often defaults to 1

# Default system prompt. Will have date and user ID info appended.
system_prompt: >
  You are a helpful Discord chatbot.
  Respond conversationally. Format replies using Discord markdown.

  **Memory Instructions:**
  If you learn new, lasting information about the user or need to correct existing notes based on the conversation, include ONE of the following instructions at the VERY END of your response, after all other text:
  1. To add a new note: `[MEM_APPEND]The new note text here.`
  2. To replace an existing note: `[MEM_REPLACE:Exact old text to find]The new text to replace it with.`
  Only include ONE instruction per response, if any. Do not mention these instructions in your conversational reply.