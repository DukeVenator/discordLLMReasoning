import asyncio
from base64 import b64encode
from dataclasses import dataclass, field
from datetime import datetime as dt
import logging
from typing import Literal, Optional, List, Dict, Any, Set, Tuple
import traceback
import textwrap

import discord
import httpx
from openai import AsyncOpenAI, APIError
import google.generativeai as genai # For Google Gemini API
from google.generativeai.types import HarmCategory, HarmBlockThreshold # For safety settings
import yaml
import aiosqlite # For persistent memory

# --- Basic Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
)
log = logging.getLogger(__name__) # Use logger instance

# --- Constants ---
VISION_MODEL_TAGS = ("gpt-4", "claude-3", "gemini", "gemma", "pixtral", "mistral-small", "llava", "vision", "vl")
PROVIDERS_SUPPORTING_USERNAMES = ("openai", "x-ai") # OpenAI-compatible providers that use 'name' field

EMBED_COLOR_COMPLETE = discord.Color.dark_green()
EMBED_COLOR_INCOMPLETE = discord.Color.orange()
EMBED_COLOR_ERROR = discord.Color.red()

STREAMING_INDICATOR = " ⚪"
EDIT_DELAY_SECONDS = 1.3 # Slightly longer delay for edits
MAX_MESSAGE_NODES = 100 # Max messages to keep in *memory cache* (different from history length)
COMMAND_PREFIX = "!" # Prefix for memory commands

# --- Configuration Loading ---
def get_config(filename="config.yaml"):
    try:
        with open(filename, "r") as file:
            # --- Default Config Structure --- (Define defaults here for merging)
            default_config = {
                "bot_token": None, "client_id": None, "status_message": None,
                "max_text": 100000, "max_images": 5, "max_messages": 25,
                "use_plain_responses": False, "allow_dms": True,
                "permissions": {
                    "users": {"allowed_ids": [], "blocked_ids": []},
                    "roles": {"allowed_ids": [], "blocked_ids": []},
                    "channels": {"allowed_ids": [], "blocked_ids": []}
                },
                "providers": {}, # Provider details filled from file
                "model": "openai/gpt-4o",
                "extra_api_parameters": {"max_tokens": 4096, "temperature": 1.0},
                "system_prompt": "You are a helpful Discord chatbot.",
                "memory": { # Default memory config
                    "enabled": False,
                    "database_path": "llmcord_memory.db",
                    "prompt_injection_method": "system_prompt_prefix",
                    "memory_prefix": "[User Memory/Notes]:\n",
                    "max_memory_length": 1500,
                    "llm_suggests_memory": False,
                    "memory_suggestion_prompt": "",
                    "memory_suggestion_start_marker": "[MEM_UPDATE]",
                    "memory_suggestion_end_marker": "[/MEM_UPDATE]",
                    "memory_suggestion_mode": "append",
                    "memory_suggestion_append_prefix": "\n- "
                }
            }

            loaded_config = yaml.safe_load(file)
            if not loaded_config:
                 raise ValueError("Config file is empty or invalid.")

            # Deep merge function (simple version for two levels)
            def merge_dicts(default, loaded):
                merged = default.copy()
                for key, value in loaded.items():
                    if isinstance(value, dict) and isinstance(merged.get(key), dict):
                        merged[key] = merge_dicts(merged[key], value)
                    else:
                        merged[key] = value
                return merged

            merged_cfg = merge_dicts(default_config, loaded_config)

            # --- Config Validation ---
            if not merged_cfg.get("bot_token"):
                raise ValueError("`bot_token` is missing in config.yaml")
            if "/" not in merged_cfg.get("model", ""):
                raise ValueError("`model` format must be 'provider_name/model_name'")

            return merged_cfg

    except FileNotFoundError:
        log.critical(f"CRITICAL: Configuration file '{filename}' not found. Exiting.")
        exit()
    except Exception as e:
        log.critical(f"CRITICAL: Error loading configuration from '{filename}': {e}")
        exit()

cfg = get_config()
cfg = get_config()

# --- Memory Configuration & DB Setup ---
MEMORY_CONFIG = cfg.get("memory", {})
MEMORY_ENABLED = MEMORY_CONFIG.get("enabled", False)
MEMORY_DB_PATH = MEMORY_CONFIG.get("database_path", "llmcord_memory.db")
MEMORY_PROMPT_METHOD = MEMORY_CONFIG.get("prompt_injection_method", "system_prompt_prefix")
MEMORY_PREFIX = MEMORY_CONFIG.get("memory_prefix", "[User Memory/Notes]:\n")
MEMORY_MAX_LENGTH = MEMORY_CONFIG.get("max_memory_length", 1500)
# -- LLM Suggestion Config --
LLM_SUGGESTS_MEMORY = MEMORY_ENABLED and MEMORY_CONFIG.get("llm_suggests_memory", False)
MEMORY_SUGGESTION_PROMPT = MEMORY_CONFIG.get("memory_suggestion_prompt", "")
MEMORY_SUGGESTION_START = MEMORY_CONFIG.get("memory_suggestion_start_marker", "[MEM_UPDATE]")
MEMORY_SUGGESTION_END = MEMORY_CONFIG.get("memory_suggestion_end_marker", "[/MEM_UPDATE]")
MEMORY_SUGGESTION_MODE = MEMORY_CONFIG.get("memory_suggestion_mode", "append")
MEMORY_SUGGESTION_APPEND_PREFIX = MEMORY_CONFIG.get("memory_suggestion_append_prefix", "\n- ")
DB_CONN = None # Global variable for the database connection

# --- Memory DB Functions ---
async def init_db():
    """Initializes the SQLite database connection and creates the memory table."""
    global DB_CONN
    if not MEMORY_ENABLED: return
    try:
        DB_CONN = await aiosqlite.connect(MEMORY_DB_PATH)
        await DB_CONN.execute("""
            CREATE TABLE IF NOT EXISTS user_memory (
                user_id INTEGER PRIMARY KEY,
                memory_text TEXT,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        await DB_CONN.commit()
        log.info(f"Memory database initialized at {MEMORY_DB_PATH}")
    except Exception as e:
        log.error(f"Failed to initialize memory database: {e}")
        DB_CONN = None # Ensure connection is None if init fails

async def close_db():
    """Closes the database connection."""
    if DB_CONN:
        try:
            await DB_CONN.close()
            log.info("Memory database connection closed.")
        except Exception as e:
            log.error(f"Error closing memory database: {e}")

async def get_user_memory(user_id: int) -> str | None:
    """Retrieves the persistent memory for a given user ID."""
    if not MEMORY_ENABLED or not DB_CONN:
        return None
    try:
        async with DB_CONN.execute("SELECT memory_text FROM user_memory WHERE user_id = ?", (user_id,)) as cursor:
            row = await cursor.fetchone()
            return row[0] if row and row[0] else None # Return None if memory is empty string
    except Exception as e:
        log.error(f"Error getting memory for user {user_id}: {e}")
        return None

async def save_user_memory(user_id: int, memory_text: str):
    """Saves or updates the persistent memory for a given user ID."""
    if not MEMORY_ENABLED or not DB_CONN:
        return
    try:
        # Truncate if necessary BEFORE saving
        final_memory_text = memory_text[:MEMORY_MAX_LENGTH]
        if len(memory_text) > MEMORY_MAX_LENGTH:
             log.warning(f"Memory for user {user_id} truncated to {MEMORY_MAX_LENGTH} characters before saving.")

        await DB_CONN.execute(
            """
            INSERT INTO user_memory (user_id, memory_text, last_updated)
            VALUES (?, ?, CURRENT_TIMESTAMP)
            ON CONFLICT(user_id) DO UPDATE SET
                memory_text = excluded.memory_text,
                last_updated = CURRENT_TIMESTAMP
            """,
            (user_id, final_memory_text) # Save potentially truncated text
        )
        await DB_CONN.commit()
        log.debug(f"Saved memory for user {user_id}. Length: {len(final_memory_text)}")
    except Exception as e:
        log.error(f"Error saving memory for user {user_id}: {e}")


# --- Discord Client Setup ---
if cfg["client_id"]:
    log.info(f"\n\nBOT INVITE URL:\nhttps://discord.com/api/oauth2/authorize?client_id={cfg['client_id']}&permissions=412317273088&scope=bot\n")

intents = discord.Intents.default()
intents.message_content = True
activity = discord.CustomActivity(name=(cfg["status_message"] or "github.com/jakobdylanc/llmcord")[:128])
discord_client = discord.Client(intents=intents, activity=activity)

# --- Global Variables ---
httpx_client = httpx.AsyncClient() # For fetching attachments
msg_nodes: Dict[int, 'MsgNode'] = {} # In-memory cache of processed messages
last_task_time: float = 0 # For rate-limiting embed edits

# --- Message Node Dataclass ---
@dataclass
class MsgNode:
    text: Optional[str] = None
    images: list = field(default_factory=list) # Store as list of {'type': 'image_url', 'image_url': {'url': 'data:...'}} for OpenAI/Gemini

    role: Literal["user", "assistant", "system"] = "assistant" # Added system role possibility
    user_id: Optional[int] = None # Discord User ID if role is 'user'

    has_bad_attachments: bool = False # If message had unsupported attachments
    fetch_parent_failed: bool = False # If fetching parent message failed

    parent_msg: Optional[discord.Message] = None # Reference to parent discord.Message for traversal

    # Lock for thread safety when processing the same message concurrently (though less likely with single bot instance)
    lock: asyncio.Lock = field(default_factory=asyncio.Lock)


# --- Helper Functions ---
def is_vision_model(model_name: str) -> bool:
    """Checks if the model name suggests vision capabilities."""
    return any(tag in model_name.lower() for tag in VISION_MODEL_TAGS)

def get_llm_provider_config(provider_name: str) -> Dict[str, Any]:
    """Gets the configuration for a specific LLM provider."""
    provider_cfg = cfg["providers"].get(provider_name)
    if not provider_cfg:
        raise ValueError(f"Configuration for provider '{provider_name}' not found in config.yaml")
    return provider_cfg

def translate_to_gemini_format(messages_openai: List[Dict[str, Any]], system_prompt_text: Optional[str]) -> Tuple[Optional[str], List[Dict[str, Any]]]:
    """Translates OpenAI message format to Gemini format."""
    gemini_messages = []
    gemini_system_instruction = None

    # Handle system prompt for Gemini 1.5+
    if system_prompt_text:
        gemini_system_instruction = system_prompt_text # Use the dedicated parameter if available

    for msg in messages_openai:
        role = msg.get("role")
        content = msg.get("content")

        if role == "system":
             # If system_instruction wasn't set, prepend to first user message or handle differently
             if not gemini_system_instruction:
                 log.warning("System prompt provided but Gemini model might not support system_instruction. Prepending to first user message.")
                 # This requires modifying the *next* user message, complex state needed.
                 # Alternative: Format as a user/model turn if required by older Gemini models.
                 # For simplicity with 1.5, we rely on system_instruction.
                 pass # Ignore system message if system_instruction is used
             continue

        gemini_role = "model" if role == "assistant" else "user"
        parts = []

        if isinstance(content, str):
            parts.append({"text": content})
        elif isinstance(content, list): # Handle vision model format (list of dicts)
            for item in content:
                if item.get("type") == "text":
                    parts.append({"text": item.get("text", "")})
                elif item.get("type") == "image_url":
                    image_url_data = item.get("image_url", {}).get("url", "")
                    if image_url_data.startswith("data:"):
                        # Extract mime type and base64 data
                        try:
                            header, b64_data = image_url_data.split(",", 1)
                            mime_type = header.split(":")[1].split(";")[0]
                            parts.append({"inline_data": {"mime_type": mime_type, "data": b64_data}})
                        except Exception as e:
                            log.warning(f"Could not parse image data URI: {e}")
                    else:
                         log.warning(f"Gemini requires inline base64 image data, received URL: {image_url_data}")
                         # Attempt to add a note about the image URL? Or just skip? Skip for now.


        if parts: # Only add message if it has content parts
             gemini_messages.append({"role": gemini_role, "parts": parts})

    return gemini_system_instruction, gemini_messages


# --- Discord Event Handler ---
@discord_client.event
async def on_ready():
    """Called when the bot is ready."""
    log.info(f'Logged in as {discord_client.user.name} ({discord_client.user.id})')
    log.info(f'Memory Enabled: {MEMORY_ENABLED}')
    await init_db() # Initialize DB when bot is ready

@discord_client.event
async def on_message(new_msg: discord.Message):
    """Handles incoming messages."""
    global msg_nodes, last_task_time

    # --- 1. Basic Checks & Permissions ---
    if new_msg.author.bot: return # Ignore bots

    # --- Memory Command Handling ---
    if MEMORY_ENABLED and new_msg.content.startswith(COMMAND_PREFIX):
        parts = new_msg.content.split(maxsplit=1)
        command = parts[0][len(COMMAND_PREFIX):].lower()
        args = parts[1] if len(parts) > 1 else None

        if command == "memory":
            await handle_memory_command(new_msg, args)
            return # Stop processing as command handled
        elif command == "forget":
            await handle_forget_command(new_msg)
            return # Stop processing as command handled
        # Allow other commands or fall through if not a memory command

    # --- Regular Message Handling ---
    is_dm = new_msg.channel.type == discord.ChannelType.private
    if not is_dm and discord_client.user not in new_msg.mentions and not new_msg.reference:
        # Only respond to DMs, mentions, or replies in channels
        # (Removed reference check needing msg_nodes access early)
        return


    # --- Permission Checks ---
    role_ids = {role.id for role in getattr(new_msg.author, "roles", ())}
    channel_ids = {new_msg.channel.id, getattr(new_msg.channel, "parent_id", None), getattr(new_msg.channel, "category_id", None)} - {None}
    allow_dms = cfg["allow_dms"]
    permissions = cfg["permissions"]

    allowed_users, blocked_users = set(permissions["users"]["allowed_ids"]), set(permissions["users"]["blocked_ids"])
    allowed_roles, blocked_roles = set(permissions["roles"]["allowed_ids"]), set(permissions["roles"]["blocked_ids"])
    allowed_channels, blocked_channels = set(permissions["channels"]["allowed_ids"]), set(permissions["channels"]["blocked_ids"])

    is_blocked_user = new_msg.author.id in blocked_users or any(role_id in blocked_roles for role_id in role_ids)
    is_specifically_allowed_user = new_msg.author.id in allowed_users or any(role_id in allowed_roles for role_id in role_ids)
    is_generally_allowed_user = not allowed_users and not allowed_roles
    is_allowed_user = not is_blocked_user and (is_specifically_allowed_user or is_generally_allowed_user)

    is_blocked_channel = any(channel_id in blocked_channels for channel_id in channel_ids)
    is_specifically_allowed_channel = any(channel_id in allowed_channels for channel_id in channel_ids)
    is_generally_allowed_channel = not allowed_channels
    is_allowed_channel = not is_blocked_channel and (is_specifically_allowed_channel or is_generally_allowed_channel)

    if not is_allowed_user or (not is_dm and not is_allowed_channel) or (is_dm and not allow_dms):
        log.debug(f"Ignoring message {new_msg.id} due to permissions. User allowed: {is_allowed_user}, Channel allowed: {is_allowed_channel if not is_dm else 'DM'}, DMs allowed: {allow_dms}")
        return

    # --- 2. Prepare for LLM Call ---
    try:
        provider, model_name = cfg["model"].split("/", 1)
        provider_cfg = get_llm_provider_config(provider)
        api_key = provider_cfg.get("api_key") # Might be None for local models

        is_gemini = provider == "google-gemini"
        openai_client = None
        gemini_model = None

        if is_gemini:
            if not api_key:
                 log.error(f"API key for google-gemini provider is missing in config.")
                 await new_msg.reply("⚠️ Configuration error: Google Gemini API key is missing.", mention_author=False)
                 return
            try:
                genai.configure(api_key=api_key)
                # Configure safety settings (optional, example: block less)
                safety_settings = {
                     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                 }
                gemini_model = genai.GenerativeModel(model_name=model_name, safety_settings=safety_settings)
                log.info(f"Using Google Gemini model: {model_name}")
            except Exception as e:
                 log.error(f"Failed to configure Google Gemini client: {e}")
                 await new_msg.reply(f"⚠️ Error configuring Google Gemini: {e}", mention_author=False)
                 return
        else: # OpenAI compatible
            base_url = provider_cfg.get("base_url")
            if not base_url:
                 log.error(f"Base URL for {provider} provider is missing in config.")
                 await new_msg.reply(f"⚠️ Configuration error: Base URL for {provider} is missing.", mention_author=False)
                 return
            openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key or "sk-no-key-required", http_client=httpx_client)
            log.info(f"Using OpenAI-compatible provider: {provider}, model: {model_name}, base_url: {base_url}")

        accept_images = is_vision_model(model_name)
        accept_usernames = any(x in provider.lower() for x in PROVIDERS_SUPPORTING_USERNAMES) and not is_gemini # Gemini doesn't use 'name'

        max_text = cfg["max_text"]
        max_images = cfg["max_images"] if accept_images else 0
        max_messages = cfg["max_messages"]

        use_plain_responses = cfg["use_plain_responses"]
        max_message_length = 2000 if use_plain_responses else (4096 - len(STREAMING_INDICATOR)) # Embed description limit
    except Exception as e:
        log.error(f"Error during LLM setup for message {new_msg.id}: {e}\n{traceback.format_exc()}")
        await new_msg.reply(f"⚠️ An unexpected error occurred while setting up the language model: {e}", mention_author=False)

        # --- 3. Build Message Chain & Process Nodes ---
        messages_openai_fmt: List[Dict[str, Any]] = [] # Use OpenAI format initially for consistency
        user_warnings: Set[str] = set()
        curr_msg: Optional[discord.Message] = new_msg

        while curr_msg is not None and len(messages_openai_fmt) < max_messages:
            is_new_node = False
            # Use setdefault for atomicity, though lock handles concurrent processing
            curr_node = msg_nodes.setdefault(curr_msg.id, MsgNode())

            async with curr_node.lock:
                if curr_node.text is None: # Process message only if not already processed
                    is_new_node = True
                    log.debug(f"Processing new message node for ID: {curr_msg.id}")
                    # Remove mention
                    cleaned_content = curr_msg.content
                    if not is_dm and discord_client.user in curr_msg.mentions:
                        cleaned_content = cleaned_content.replace(discord_client.user.mention, "").lstrip()

                    # Process attachments
                    curr_node.images = []
                    text_from_attachments = []
                    num_unsupported_attachments = 0
                    if curr_msg.attachments:
                        attachment_tasks = []
                        valid_attachments = []
                        for att in curr_msg.attachments:
                            if att.content_type:
                                if att.content_type.startswith("text"):
                                    valid_attachments.append(att)
                                    attachment_tasks.append(httpx_client.get(att.url, timeout=10))
                                elif accept_images and att.content_type.startswith("image/"):
                                    valid_attachments.append(att)
                                    attachment_tasks.append(httpx_client.get(att.url, timeout=15)) # Longer timeout for images
                                else:
                                    num_unsupported_attachments += 1
                            else:
                                 num_unsupported_attachments += 1

                        if attachment_tasks:
                            attachment_responses = await asyncio.gather(*attachment_tasks, return_exceptions=True)

                            for att, resp in zip(valid_attachments, attachment_responses):
                                if isinstance(resp, Exception):
                                    log.warning(f"Failed to fetch attachment {att.filename}: {resp}")
                                    num_unsupported_attachments += 1
                                    continue
                                if resp.status_code != 200:
                                     log.warning(f"Failed to fetch attachment {att.filename}: Status {resp.status_code}")
                                     num_unsupported_attachments += 1
                                     continue

                                if att.content_type.startswith("text"):
                                    try:
                                        text_from_attachments.append(resp.text)
                                    except Exception as e:
                                        log.warning(f"Could not decode text attachment {att.filename}: {e}")
                                        num_unsupported_attachments += 1
                                elif accept_images and att.content_type.startswith("image/"):
                                    try:
                                        b64_img = b64encode(resp.content).decode('utf-8')
                                        curr_node.images.append({
                                            "type": "image_url",
                                            "image_url": {"url": f"data:{att.content_type};base64,{b64_img}"}
                                        })
                                    except Exception as e:
                                         log.warning(f"Could not process image attachment {att.filename}: {e}")
                                         num_unsupported_attachments += 1

                    curr_node.has_bad_attachments = num_unsupported_attachments > 0

                    # Combine text content
                    embed_texts = ["\n".join(filter(None, (embed.title, embed.description, getattr(embed.footer, 'text', None)))) for embed in curr_msg.embeds]
                    curr_node.text = "\n".join(
                        ([cleaned_content] if cleaned_content else [])
                        + embed_texts
                        + text_from_attachments
                    )

                    # Set role and user ID
                    curr_node.role = "assistant" if curr_msg.author == discord_client.user else "user"
                    curr_node.user_id = curr_msg.author.id if curr_node.role == "user" else None

                    # --- Find Parent Message (Improved Logic) ---
                    parent_msg_obj: Optional[discord.Message] = None
                    try:
                         # 1. Direct Reply?
                         if curr_msg.reference and curr_msg.reference.message_id:
                             try:
                                 parent_msg_obj = curr_msg.reference.cached_message or await curr_msg.channel.fetch_message(curr_msg.reference.message_id)
                             except (discord.NotFound, discord.Forbidden):
                                 log.warning(f"Could not fetch replied-to message {curr_msg.reference.message_id}")
                                 curr_node.fetch_parent_failed = True

                         # 2. Public Thread Start? (Parent is the starter message in parent channel)
                         elif curr_msg.channel.type == discord.ChannelType.public_thread and isinstance(curr_msg.channel, discord.Thread):
                             if curr_msg.id == curr_msg.channel.id: # Is this the very first message?
                                 try:
                                     # Starter message might not be fetched yet
                                     parent_msg_obj = curr_msg.channel.starter_message or await curr_msg.channel.parent.fetch_message(curr_msg.channel.id)
                                 except (discord.NotFound, discord.Forbidden):
                                     log.warning(f"Could not fetch starter message for thread {curr_msg.channel.id}")
                                     curr_node.fetch_parent_failed = True

                         # 3. Implicit DM continuation? (Message right before was from the bot)
                         elif is_dm and not curr_msg.reference:
                              try:
                                   async for prev_msg in curr_msg.channel.history(before=curr_msg, limit=1):
                                        if prev_msg.author == discord_client.user:
                                             parent_msg_obj = prev_msg
                                        break # Only check the immediately preceding message
                              except (discord.Forbidden, discord.HTTPException):
                                   log.warning(f"Could not check history in DM with {new_msg.author.id}")

                         # 4. Implicit Channel continuation? (Message right before from same user, if not replying/mentioning)
                         # This logic seems less reliable, might grab unrelated messages. Let's disable for now.
                         # elif not is_dm and not curr_msg.reference and discord_client.user not in curr_msg.mentions:
                         #      # Check if message immediately before is from same user
                         #      pass

                         curr_node.parent_msg = parent_msg_obj

                    except Exception as e:
                         log.exception(f"Unexpected error finding parent for message {curr_msg.id}")
                         curr_node.fetch_parent_failed = True
                    # End of parent finding

                # --- Prepare message content for API ---
                api_content: Any = ""
                # Limit images and text
                images_to_send = curr_node.images[:max_images]
                text_to_send = (curr_node.text or "")[:max_text] # Ensure text is string

                if images_to_send and accept_images:
                    # OpenAI/Gemini format: list of dicts for multimodal
                    content_parts = []
                    if text_to_send:
                         content_parts.append({"type": "text", "text": text_to_send})
                    content_parts.extend(images_to_send) # Assumes images are already in correct dict format
                    api_content = content_parts
                else:
                    api_content = text_to_send # Just text

                # Add to chain if content exists
                if api_content:
                    message_dict = {"role": curr_node.role, "content": api_content}
                    if accept_usernames and curr_node.user_id is not None: # Add 'name' if supported by provider (and not Gemini)
                        message_dict["name"] = str(curr_node.user_id)

                    messages_openai_fmt.append(message_dict)
                elif is_new_node: # Log if a new node ended up empty
                     log.debug(f"Message node {curr_msg.id} resulted in empty content after processing.")


                # --- Add User Warnings ---
                if len(curr_node.text or "") > max_text:
                    user_warnings.add(f"⚠️ Max {max_text:,} characters/message")
                if len(curr_node.images) > max_images:
                    user_warnings.add(f"⚠️ Max {max_images} image{'' if max_images == 1 else 's'}/message" if max_images > 0 else "⚠️ Can't see images")
                if curr_node.has_bad_attachments:
                    user_warnings.add("⚠️ Unsupported attachments ignored")
                if curr_node.fetch_parent_failed:
                     user_warnings.add(f"⚠️ Couldn't link full conversation history")
                elif curr_node.parent_msg is not None and len(messages_openai_fmt) == max_messages:
                    user_warnings.add(f"⚠️ Using last {len(messages_openai_fmt)} message{'' if len(messages_openai_fmt) == 1 else 's'}")

                # --- Move to parent message ---
                curr_msg = curr_node.parent_msg
            # End of async with curr_node.lock

        # --- 4. Add System Prompt & Memory ---
        system_prompt_text = cfg.get("system_prompt", "").strip()
        final_system_prompt = ""
        if system_prompt_text:
            system_prompt_extras = [f"Today's date: {dt.now().strftime('%B %d %Y')}."]
            if accept_usernames: # OpenAI-compatible 'name' field
                system_prompt_extras.append("User IDs may be provided in the 'name' field for user messages.")
            elif not is_gemini: # Other models might need explicit mention instruction
                 system_prompt_extras.append("User's names are their Discord IDs. If you need to mention a user, use the format '<@USER_ID>'.")

            final_system_prompt = "\n".join([system_prompt_text] + system_prompt_extras)

        # --- Inject Memory ---
        user_memory = None
        if MEMORY_ENABLED:
            user_memory = await get_user_memory(new_msg.author.id)
            if user_memory:
                 log.debug(f"Retrieved memory for user {new_msg.author.id}: {user_memory[:100]}...")
                 if MEMORY_PROMPT_METHOD == "system_prompt_prefix":
                     final_system_prompt = f"{MEMORY_PREFIX}{user_memory}\n\n{final_system_prompt}" if final_system_prompt else f"{MEMORY_PREFIX}{user_memory}"

        # --- Inject LLM Suggestion Prompt ---
        if LLM_SUGGESTS_MEMORY and MEMORY_SUGGESTION_PROMPT:
            final_system_prompt = (final_system_prompt.strip() + "\n\n" + MEMORY_SUGGESTION_PROMPT.strip()).strip()

        # Add final system prompt to OpenAI format list (will be handled by Gemini translation if needed)
        if final_system_prompt:
             messages_openai_fmt.append({"role": "system", "content": final_system_prompt})

        # Inject memory as user message if configured
        if user_memory and MEMORY_PROMPT_METHOD == "user_message_prefix":
             messages_openai_fmt.insert(0, {"role": "user", "content": f"{MEMORY_PREFIX}{user_memory}"}) # Insert near beginning


        # Reverse messages to chronological order for API call
        messages_openai_fmt.reverse()

        # --- 5. Prepare API Call Specifics ---
        response_content_full = "" # Store the full response including potential suggestions
        response_msgs: List[discord.Message] = [] # Track sent Discord message objects
        response_contents: List[str] = [] # Track content strings for multi-part responses
        finish_reason: Optional[str] = None
        edit_task: Optional[asyncio.Task] = None

        # Prepare embed for warnings
        embed = discord.Embed()
        if user_warnings:
             # Add warnings as fields (better than description spam)
             # embed.title = "⚠️ Input Warnings"
             for warning in sorted(user_warnings):
                  embed.add_field(name=warning, value="", inline=False)
             embed.color = EMBED_COLOR_INCOMPLETE # Orange for warnings

        # --- 6. Execute API Call & Handle Streaming ---
        log.info(f"Sending request to {provider}/{model_name} (History: {len(messages_openai_fmt)} msgs). User: {new_msg.author.id}")
        # log.debug(f"API Messages: {messages_openai_fmt}") # Can be very verbose

        try:
            async with new_msg.channel.typing():

                # --- Gemini API Call ---
                if is_gemini and gemini_model:
                    gemini_system_instruction, gemini_messages = translate_to_gemini_format(messages_openai_fmt, final_system_prompt)
                    log.debug(f"Gemini SystemInstruction: {gemini_system_instruction}")
                    log.debug(f"Gemini Messages: {gemini_messages}")

                    generation_config_args = cfg["extra_api_parameters"]

                    # Translate common OpenAI params if present
                    gemini_gen_config = {
                        "max_output_tokens": generation_config_args.pop("max_tokens", None),
                        "temperature": generation_config_args.pop("temperature", None),
                        "top_p": generation_config_args.pop("top_p", None),
                        # Add other specific Gemini translations if needed
                        **generation_config_args # Pass remaining unmodified args
                    }
                    # Filter out None values
                    gemini_gen_config = {k: v for k, v in gemini_gen_config.items() if v is not None}

                    gemini_kwargs = {
                        "contents": gemini_messages,
                        "stream": True,
                        "generation_config": genai.types.GenerationConfig(**gemini_gen_config)
                    }
                    if gemini_system_instruction:
                         gemini_kwargs["system_instruction"] = gemini_system_instruction

                    try:
                        stream = await asyncio.to_thread(gemini_model.generate_content, **gemini_kwargs)

                        # --- Gemini Streaming Loop ---
                        async for chunk in stream:
                            # TODO: How to get finish_reason reliably from Gemini stream?
                            # Check chunk.prompt_feedback for blocks?
                            if chunk.prompt_feedback.block_reason:
                                 finish_reason = f"Blocked: {chunk.prompt_feedback.block_reason}"
                                 log.warning(f"Gemini response blocked: {finish_reason}")
                                 break

                            chunk_text = chunk.text
                            response_content_full += chunk_text # Accumulate full response

                            # --- Handle Discord message updates (common logic) ---
                            response_msgs, response_contents, edit_task, last_task_time = await update_discord_response(
                                new_msg, chunk_text, finish_reason, response_msgs, response_contents, embed, edit_task, last_task_time, use_plain_responses, max_message_length
                            )
                        # Check finish reason after loop (may need response object if stream doesn't provide it)
                        # finish_reason = getattr(stream._response, 'candidates', [None])[0].finish_reason # Example, might change

                    except Exception as e:
                         log.error(f"Google Gemini API Error: {e}\n{traceback.format_exc()}")
                         # Attempt to get specific error details if available
                         error_details = str(e)
                         embed.color = EMBED_COLOR_ERROR
                         embed.title="⚠️ Gemini API Error"
                         embed.description = f"```\n{error_details[:1900]}\n```" # Show error in embed
                         await new_msg.reply(embed=embed, mention_author=False)
                         return # Stop processing on API error


                # --- OpenAI Compatible API Call ---
                elif openai_client:
                    openai_kwargs = dict(
                        model=model_name,
                        messages=messages_openai_fmt,
                        stream=True,
                        **cfg["extra_api_parameters"] # Pass extra params directly
                    )
                    try:
                        stream = await openai_client.chat.completions.create(**openai_kwargs)

                        # --- OpenAI Streaming Loop ---
                        async for chunk in stream:
                            if finish_reason is not None: break # Stop if finished

                            finish_reason = chunk.choices[0].finish_reason
                            chunk_text = chunk.choices[0].delta.content or ""
                            response_content_full += chunk_text # Accumulate full response

                            # --- Handle Discord message updates (common logic) ---
                            response_msgs, response_contents, edit_task, last_task_time = await update_discord_response(
                                new_msg, chunk_text, finish_reason, response_msgs, response_contents, embed, edit_task, last_task_time, use_plain_responses, max_message_length
                            )

                    except APIError as e:
                         log.error(f"OpenAI API Error: {e}")
                         embed.color = EMBED_COLOR_ERROR
                         embed.title=f"⚠️ OpenAI API Error ({provider})"
                         embed.description = f"```\n{e}\n```"
                         await new_msg.reply(embed=embed, mention_author=False)
                         return
                    except Exception as e:
                         log.error(f"OpenAI Compatible API Error: {e}\n{traceback.format_exc()}")
                         embed.color = EMBED_COLOR_ERROR
                         embed.title=f"⚠️ API Error ({provider})"
                         embed.description = f"```\n{e}\n```"
                         await new_msg.reply(embed=embed, mention_author=False)
                         return

                # --- 7. Final Processing After Stream ---
                if edit_task is not None and not edit_task.done():
                    await edit_task # Ensure last edit task is complete

                # --- Process LLM-suggested memory update ---
                final_display_content = response_content_full # Start with full, may be modified
                suggested_memory_update = None

                if LLM_SUGGESTS_MEMORY and response_msgs: # Check if feature enabled and we got a response
                    log.debug(f"Checking for memory suggestion in full response (length {len(response_content_full)})")
                    try:
                        start_idx = response_content_full.rfind(MEMORY_SUGGESTION_START) # Last occurrence
                        if start_idx != -1:
                            end_idx = response_content_full.find(MEMORY_SUGGESTION_END, start_idx)
                            if end_idx != -1:
                                suggestion = response_content_full[start_idx + len(MEMORY_SUGGESTION_START):end_idx].strip()
                                log.info(f"LLM suggested memory update for user {new_msg.author.id}: '{suggestion}'")

                                if suggestion: # Only process if suggestion is not empty
                                    suggested_memory_update = suggestion

                                    # Remove suggestion from final displayed content
                                    content_before = response_content_full[:start_idx].rstrip()
                                    content_after = response_content_full[end_idx + len(MEMORY_SUGGESTION_END):].lstrip()
                                    final_display_content = (content_before + content_after).strip()

                                    # Perform final edit on the *last* Discord message to remove suggestion
                                    last_response_msg = response_msgs[-1]
                                    if use_plain_responses:
                                         # Edit plain text message content
                                         if last_response_msg.content != final_display_content:
                                              await last_response_msg.edit(content=final_display_content[:2000]) # Apply limit
                                    else:
                                         # Edit embed description
                                         embed.description = final_display_content[:4096] # Apply limit
                                         embed.color = EMBED_COLOR_COMPLETE # Mark as complete
                                         if STREAMING_INDICATOR in embed.description: # Should be removed by logic above, but double check
                                               embed.description = embed.description.replace(STREAMING_INDICATOR, "")
                                         await last_response_msg.edit(embed=embed)
                                    log.debug(f"Performed final edit on msg {last_response_msg.id} to remove memory suggestion.")

                    except Exception as e:
                        log.error(f"Error processing LLM memory suggestion parsing for user {new_msg.author.id}: {e}")
                        # Don't crash, just log. Displayed content might contain markers.

                # --- Save the suggested memory update (if found) ---
                if suggested_memory_update:
                     await process_and_save_suggestion(new_msg.author.id, suggested_memory_update)


        except Exception as e:
             log.exception("Error during message processing or LLM call")
             embed.color = EMBED_COLOR_ERROR
             embed.title="⚠️ An Unexpected Error Occurred"
             embed.description = f"```\n{e}\n```"
             try:
                 await new_msg.reply(embed=embed, mention_author=False)
             except Exception: # If reply fails too
                 pass # Avoid error loops

        finally:
            # --- 8. Update Node Cache & Cleanup ---
            # Update cache with *final* content after edits/suggestion removal
            final_combined_content = "".join(response_contents) # Get base content
            if suggested_memory_update: # Use the cleaned content if suggestion was removed
                 final_combined_content = final_display_content

            for response_msg in response_msgs:
                 if response_msg.id in msg_nodes:
                     async with msg_nodes[response_msg.id].lock:
                         msg_nodes[response_msg.id].text = final_combined_content # Store final text
                         # We didn't process images for bot response, keep empty
                         # Ensure lock is released after update
                 else:
                     # Should have been added during update_discord_response, log if missing
                     log.warning(f"Response message {response_msg.id} not found in msg_nodes after completion.")

            # --- Cache Cleaning ---
            if (num_nodes := len(msg_nodes)) > MAX_MESSAGE_NODES:
                log.info(f"Cache size ({num_nodes}) exceeds max ({MAX_MESSAGE_NODES}). Pruning...")
                # Sort by message ID (proxy for time) and remove oldest
                ids_to_remove = sorted(msg_nodes.keys())[: num_nodes - MAX_MESSAGE_NODES]
                for msg_id in ids_to_remove:
                    if msg_id in msg_nodes: # Check again in case of race condition (unlikely)
                        async with msg_nodes[msg_id].lock: # Acquire lock before removing
                            removed_node = msg_nodes.pop(msg_id, None)
                            if removed_node:
                                 log.debug(f"Removed node {msg_id} from cache.")
                log.info(f"Cache pruned. New size: {len(msg_nodes)}")


# --- Helper for Discord Response Update ---
async def update_discord_response(
    new_msg: discord.Message,
    chunk_text: str,
    finish_reason: Optional[str],
    response_msgs: List[discord.Message],
    response_contents: List[str],
    embed: discord.Embed,
    edit_task: Optional[asyncio.Task],
    last_task_time: float,
    use_plain_responses: bool,
    max_message_length: int
) -> Tuple[List[discord.Message], List[str], Optional[asyncio.Task], float]:
    """Handles creating/editing Discord messages during streaming."""

    if not chunk_text and finish_reason is None: # Skip empty updates unless it's the final one
        return response_msgs, response_contents, edit_task, last_task_time

    start_next_msg = False
    current_content_part = response_contents[-1] if response_contents else ""

    if not response_contents or len(current_content_part + chunk_text) > max_message_length:
        start_next_msg = True
        response_contents.append(chunk_text) # Start new content part
    else:
        response_contents[-1] += chunk_text # Append to current part

    if use_plain_responses:
        # Plain responses are sent only at the end or when max length is hit
        if finish_reason is not None or start_next_msg:
            content_to_send = response_contents[-2] if start_next_msg else response_contents[-1]
            if content_to_send: # Only send if there's content
                reply_to_msg = new_msg if not response_msgs else response_msgs[-1]
                response_msg = await reply_to_msg.reply(content=content_to_send[:2000], suppress_embeds=True, mention_author=False) # Apply limit just in case
                response_msgs.append(response_msg)
                # Add node immediately for plain responses
                if response_msg.id not in msg_nodes:
                    msg_nodes[response_msg.id] = MsgNode(parent_msg=new_msg, role="assistant") # Lock acquired later
    else:
        # Embed streaming updates
        now = dt.now().timestamp()
        # Edit conditions: rate limit passed, starting new msg, final chunk, or forced by length overflow
        is_final_chunk = finish_reason is not None
        ready_to_edit = (edit_task is None or edit_task.done()) and (now - last_task_time >= EDIT_DELAY_SECONDS)
        if start_next_msg or ready_to_edit or is_final_chunk:
            if edit_task is not None: # Ensure previous edit finished
                 await edit_task

            # Use the content part *currently being built* for the embed description
            display_text = response_contents[-1]
            embed_desc = display_text[:max_message_length] # Apply embed description limit

            is_truly_complete = finish_reason is not None and finish_reason.lower() in ("stop", "length", "end_turn") # Consider 'length' as complete

            if not is_final_chunk:
                 embed_desc += STREAMING_INDICATOR

            embed.description = embed_desc
            embed.color = EMBED_COLOR_COMPLETE if is_truly_complete else EMBED_COLOR_INCOMPLETE

            if start_next_msg and response_contents[-1]: # Only create new message if there's content for it
                 reply_to_msg = new_msg if not response_msgs else response_msgs[-1]
                 try:
                     response_msg = await reply_to_msg.reply(embed=embed, silent=True, mention_author=False)
                     response_msgs.append(response_msg)
                     # Add node immediately for embed responses too
                     if response_msg.id not in msg_nodes:
                         msg_nodes[response_msg.id] = MsgNode(parent_msg=new_msg, role="assistant") # Lock acquired later
                 except discord.HTTPException as e:
                      log.error(f"Failed to send initial reply message: {e}")
                      # Decide how to handle - maybe stop stream?
                      raise # Re-raise to stop processing this message
            elif response_msgs: # Edit existing message
                 edit_task = asyncio.create_task(response_msgs[-1].edit(embed=embed))

            last_task_time = now

    return response_msgs, response_contents, edit_task, last_task_time


# --- Memory Command Handlers ---
async def handle_memory_command(message: discord.Message, args: Optional[str]):
    """Handles the !memory command."""
    if not MEMORY_ENABLED:
        await message.reply("Memory feature is disabled.", mention_author=False, delete_after=10)
        return
    if not DB_CONN:
        await message.reply("Memory database is not connected.", mention_author=False, delete_after=10)
        return

    user_id = message.author.id
    target_user_id = user_id # TODO: Allow admins to view/edit others? For now, only self.

    if args is None:
        # View memory
        current_memory = await get_user_memory(target_user_id)
        if current_memory:
            reply_content = f"Your current notes ({len(current_memory)} chars / {MEMORY_MAX_LENGTH} max):\n```\n{current_memory}\n```"
            try:
                 # Send as ephemeral if possible (requires interaction context, not just message)
                 # Hacky: Try sending normally and deleting user command? Or just send publicly.
                 # Send publicly for now, maybe add ephemeral later via slash commands
                 await message.reply(content=reply_content[:2000], mention_author=False)
                 # Consider deleting user's !memory command message for privacy if channel allows
                 # try: await message.delete() except: pass
            except discord.HTTPException as e:
                 log.warning(f"Failed to send memory view: {e}")
                 await message.reply(f"Error displaying memory: {e}", mention_author=False)

        else:
            await message.reply("You have no saved notes.", mention_author=False)
    else:
        # Update memory
        if len(args) > MEMORY_MAX_LENGTH:
             await message.reply(f"❌ Error: Notes too long (max {MEMORY_MAX_LENGTH} chars). **Not saved.**", mention_author=False)
             return

        await save_user_memory(target_user_id, args)
        await message.reply(f"✅ Your notes have been updated ({len(args)} chars saved).", mention_author=False)
        # Consider deleting user's !memory command message
        # try: await message.delete() except: pass

async def handle_forget_command(message: discord.Message):
    """Handles the !forget command."""
    if not MEMORY_ENABLED:
        await message.reply("Memory feature is disabled.", mention_author=False, delete_after=10)
        return
    if not DB_CONN:
        await message.reply("Memory database is not connected.", mention_author=False, delete_after=10)
        return

    user_id = message.author.id
    target_user_id = user_id # Only self for now

    await save_user_memory(target_user_id, "") # Save empty string to clear
    await message.reply("✅ Your notes have been cleared.", mention_author=False)
     # Consider deleting user's !forget command message
    # try: await message.delete() except: pass


async def process_and_save_suggestion(user_id: int, suggestion: str):
     """Appends or replaces user memory based on LLM suggestion and config."""
     if not suggestion: return
     try:
         current_memory = await get_user_memory(user_id)
         new_memory_text = ""

         if MEMORY_SUGGESTION_MODE == "replace":
              new_memory_text = suggestion # Replace entire memory
         else: # Append mode (default)
              # Avoid adding prefix if current memory is empty or just whitespace
              prefix = MEMORY_SUGGESTION_APPEND_PREFIX if current_memory and current_memory.strip() else ""
              new_memory_text = (current_memory if current_memory else "") + prefix + suggestion

         # Check length *after* combining/replacing
         if len(new_memory_text) > MEMORY_MAX_LENGTH:
             log.warning(f"Memory for user {user_id} exceeded max length ({MEMORY_MAX_LENGTH}) after suggestion. Implementing truncation strategy.")
             # Simple truncation: Keep the newest N characters
             new_memory_text = new_memory_text[-MEMORY_MAX_LENGTH:]
             # Alternative: Could try removing lines from the *beginning* until it fits.
             # lines = new_memory_text.splitlines()
             # while len("\n".join(lines)) > MEMORY_MAX_LENGTH and len(lines) > 1:
             #    lines.pop(0)
             # new_memory_text = "\n".join(lines)

         await save_user_memory(user_id, new_memory_text.strip()) # Save, ensuring no leading/trailing whitespace
         log.info(f"Saved LLM-suggested memory update for user {user_id}. Mode: '{MEMORY_SUGGESTION_MODE}'. New length: {len(new_memory_text)}")

     except Exception as e:
         log.error(f"Error saving LLM-suggested memory for user {user_id}: {e}")


# --- Main Execution ---
async def main():
    global httpx_client
    try:
        await discord_client.start(cfg["bot_token"])
    except discord.LoginFailure:
        log.critical("CRITICAL: Improper Discord token passed.")
    except Exception as e:
         log.critical(f"CRITICAL: Error starting Discord client: {e}\n{traceback.format_exc()}")
    finally:
        log.info("Shutting down...")
        await close_db() # Close DB connection
        await httpx_client.aclose() # Close httpx client
        await discord_client.close() # Close Discord client


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        log.info("Shutdown requested by user.")