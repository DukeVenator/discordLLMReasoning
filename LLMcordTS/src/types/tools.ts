// LLMcordTS/src/types/tools.ts
import OpenAI from 'openai'; // Added import for OpenAI types

/**
 * Represents the full implementation of a tool, including its execution logic.
 */
export interface ToolImplementation {
  /**
   * The unique name used by the LLM to identify and call the tool.
   * Also used internally to map tool calls to implementations.
   */
  name: string;

  /**
   * A description of the tool's purpose and functionality for the LLM.
   */
  description: string;

  /**
   * A JSON schema object describing the expected arguments for the tool,
   * conforming to the OpenAI FunctionParameters type.
   * This helps the LLM understand how to format the arguments correctly.
   * Example:
   * {
   *   type: "object",
   *   properties: {
   *     param1: { type: "string", description: "Description of param1" },
   *     param2: { type: "number", description: "Description of param2" }
   *   },
   *   required: ["param1"]
   * }
   */
  parameters: OpenAI.FunctionParameters; // Updated type from object

  /**
   * The asynchronous function that executes the tool's logic.
   * @param args - The parsed arguments object, conforming to the `parameters` schema.
   * @returns A promise that resolves with the result of the tool's execution.
   *          The result structure can vary depending on the tool.
   */
  execute(args: any): Promise<any>;
}

/**
 * Represents the definition (schema) of a tool provided to the LLM.
 * This typically excludes the actual execution logic.
 */
export type ToolDefinition = Omit<ToolImplementation, 'execute'>;

/**
 * Represents a request from the LLM to call a specific tool.
 */
export interface ToolCallRequest {
  /** The unique identifier for this specific tool call instance. */
  id: string;
  /** The name of the tool the LLM wants to call. */
  toolName: string;
  /** The arguments for the tool call, typically as a JSON string or object. */
  args: any; // Consider string | object based on provider specifics
}

/**
 * Represents the result of a tool execution, to be sent back to the LLM.
 */
export interface ToolResult {
    /** The unique identifier matching the corresponding ToolCallRequest. */
    tool_call_id: string; // Often matches the provider's expected field name
    /** The output or result generated by the tool execution. */
    output: any; // Can be string, object, etc.
    /** Optional: Indicate if the tool execution was successful. */
    is_error?: boolean;
}

// Note: The optional JSONSchema7 import/comment is removed as we now use OpenAI.FunctionParameters